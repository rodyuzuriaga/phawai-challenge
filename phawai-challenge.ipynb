{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":87814,"databundleVersionId":10024333,"sourceType":"competition"}],"dockerImageVersionId":30822,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## **1. Preparación del Notebook 🐣**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score, confusion_matrix, ConfusionMatrixDisplay, precision_recall_curve, roc_curve, roc_auc_score, classification_report\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.pipeline import Pipeline as ImbPipeline\n\n# Cargar los datos\ntrain_df = pd.read_csv('/kaggle/input/prediccion-de-sufrir-enfermedades-coronarias/train.csv')\ntest_public_df = pd.read_csv('/kaggle/input/prediccion-de-sufrir-enfermedades-coronarias/test_public.csv')\ntest_private_df = pd.read_csv('/kaggle/input/prediccion-de-sufrir-enfermedades-coronarias/test_private.csv')\n\n# Mostrar las primeras filas del dataset de entrenamiento\nprint(\"Datos de entrenamiento:\")\nprint(train_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T01:52:22.823698Z","iopub.execute_input":"2025-01-05T01:52:22.824171Z","iopub.status.idle":"2025-01-05T01:52:24.874026Z","shell.execute_reply.started":"2025-01-05T01:52:22.824130Z","shell.execute_reply":"2025-01-05T01:52:24.872160Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **2. Análisis Exploratorio de Datos (EDA)**","metadata":{}},{"cell_type":"code","source":"# Visualizar las primeras filas del dataset\nprint(\"Primeras filas del dataset de entrenamiento:\")\nprint(train_df.head())\n\n# Analizar la distribución de la variable objetivo\nprint(\"\\nDistribución de la variable objetivo:\")\nprint(train_df['CHD_OR_MI'].value_counts())\n\n# Generar estadísticas descriptivas\nprint(\"\\nEstadísticas descriptivas del dataset de entrenamiento:\")\nprint(train_df.describe())\n\n# Identificar valores nulos\nprint(\"\\nValores nulos en el dataset de entrenamiento:\")\nprint(train_df.isnull().sum())\n\n# Visualizar valores nulos con un heatmap\nplt.figure(figsize=(12, 6))\nsns.heatmap(train_df.isnull(), cbar=False, cmap='viridis')\nplt.title('Mapa de calor de valores nulos')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T01:52:32.981114Z","iopub.execute_input":"2025-01-05T01:52:32.981502Z","iopub.status.idle":"2025-01-05T01:52:43.194323Z","shell.execute_reply.started":"2025-01-05T01:52:32.981474Z","shell.execute_reply":"2025-01-05T01:52:43.193294Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **3. Preprocesamiento**\n**a) Imputación de valores nulos:**\n* Decide estrategias para cada columna (media, mediana, moda o eliminación).\n* Usa **SimpleImputer** de scikit-learn o **fillna()** de pandas.\n\n**b) Codificación de variables categóricas:**\n* Usa **OneHotEncoder** o **LabelEncoder** para convertir variables categóricas en numéricas.\n\n**c) Escalado:**\n* Aplica escalado con **StandardScaler** o **MinMaxScaler** para las columnas numéricas si es necesario.\n\n**d) Selección de características:**\n* Realiza un análisis de correlación usando **df.corr()**.\n* Usa métodos como **SelectKBest** o algoritmos basados en árboles para determinar características importantes.","metadata":{}},{"cell_type":"code","source":"# Definir las columnas numéricas y categóricas\nnumeric_features = ['AGE', 'PHYSICAL_HEALTH', 'MENTAL_HEALTH', 'BMI']\ncategorical_features = ['SEX', 'HEALTH', 'PHYSICAL_ACTIVITIES', 'BLOOD_PRESSURE', 'HIGH_CHOLESTEROL', \n                        'SKIN_CANCER', 'CANCER', 'BRONCHITIS', 'DEPRESSIVE_DISORDER', 'KIDNEY_DISEASE', \n                        'DIABETES', 'ARTHRITIS', 'DIFFICULTY_WALKING', 'SMOKE', 'TABACCO_PRODUCTS', \n                        'E_CIGARETTES', 'HEAVY_DRINKERS', 'HIV', 'FRUITS', 'VEGETABLES', 'FRIED_POTATOES', \n                        'ETHNICITY']\n\n# Crear transformadores para las columnas numéricas y categóricas\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', StandardScaler())\n])\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Combinar los transformadores en un ColumnTransformer\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)\n    ])\n\n# Crear un pipeline con preprocesamiento y SMOTE\npipeline = ImbPipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('smote', SMOTE(random_state=42))\n])\n\n# Separar características y variable objetivo\nX = train_df.drop(columns=['CHD_OR_MI', 'ID'])\ny = train_df['CHD_OR_MI']\n\n# Preprocesar los datos\nX_resampled, y_resampled = pipeline.fit_resample(X, y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T01:53:12.565650Z","iopub.execute_input":"2025-01-05T01:53:12.566039Z","iopub.status.idle":"2025-01-05T01:53:18.232696Z","shell.execute_reply.started":"2025-01-05T01:53:12.566011Z","shell.execute_reply":"2025-01-05T01:53:18.231343Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **4. Balanceo de Clases**\nImplementa técnicas como:\n\n* Submuestreo de la clase mayoritaria.\n* Sobremuestreo con SMOTE (imbalanced-learn).\n* Aplicación de ponderación en la función de pérdida del modelo.\n\nYa hemos aplicado SMOTE en el paso anterior.","metadata":{}},{"cell_type":"markdown","source":"## **5. Entrenamiento del Modelo**\n**a) Aqui dividimos los datos de entrenamiento y validación usando** ***train_test_split()***.\n\n**b) Prueba de varios modelos:**\n* Árboles de decisión, Random Forest, Gradient Boosting, XGBoost, LightGBM, etc.\n  \n**c) Evaluamos el modelo mediante la métrica F1-Score.**\n\n**d) Por último realiza la optimización de hiperparámetros con Grid Search o Random Search. Esto es crucial para maximizar el redimiento del modelo y así encontrar la mejor combinación.**","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import cross_val_score\n\n# Definir los modelos\nmodels = {\n    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n    'Decision Tree': DecisionTreeClassifier(random_state=42),\n    'Naive Bayes': GaussianNB(),\n    'Random Forest': RandomForestClassifier(random_state=42),\n    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n}\n\n# Evaluar cada modelo usando validación cruzada\nfor name, model in models.items():\n    scores = cross_val_score(model, X_resampled, y_resampled, cv=5, scoring='f1')\n    print(f\"{name} F1-score promedio: {scores.mean()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T01:53:35.791552Z","iopub.execute_input":"2025-01-05T01:53:35.791980Z","iopub.status.idle":"2025-01-05T02:07:47.304211Z","shell.execute_reply.started":"2025-01-05T01:53:35.791950Z","shell.execute_reply":"2025-01-05T02:07:47.302507Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **6. Ensamblaje de modelos**\nLa idea es que al combinar varios modelos, se pueden compensar las debilidades de unos con las fortalezas de otros, logrando un modelo más robusto y preciso.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\n\n# Crear un ensamblaje de modelos\nensemble_model = VotingClassifier(estimators=[\n    ('lr', LogisticRegression(max_iter=1000, random_state=42)),\n    ('rf', RandomForestClassifier(random_state=42)),\n    ('xgb', XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42))\n], voting='soft')\n\n# Evaluar el ensamblaje usando validación cruzada\nensemble_scores = cross_val_score(ensemble_model, X_resampled, y_resampled, cv=5, scoring='f1')\nprint(\"Ensamblaje F1-score promedio:\", ensemble_scores.mean())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T02:09:36.558838Z","iopub.execute_input":"2025-01-05T02:09:36.559363Z","iopub.status.idle":"2025-01-05T02:22:47.167729Z","shell.execute_reply.started":"2025-01-05T02:09:36.559329Z","shell.execute_reply":"2025-01-05T02:22:47.166460Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **7. Generación de Predicciones**","metadata":{}},{"cell_type":"code","source":"# Ajustar el modelo ensamblado con todos los datos de entrenamiento\nensemble_model.fit(X_resampled, y_resampled)\n\n# Preprocesar el conjunto de prueba\ntest_public_df_preprocessed = test_public_df.drop(columns=['ID', 'CHD_OR_MI'], errors='ignore')\ntest_public_df_imputed = preprocessor.transform(test_public_df_preprocessed)\n\n# Hacer predicciones en el conjunto de prueba\npredictions = ensemble_model.predict(test_public_df_imputed)\n\n# Crear el archivo de envío\nsubmission = pd.DataFrame({\n    'ID': test_public_df['ID'],\n    'CHD_OR_MI': predictions\n})\n\n# Guardar el archivo de envío\nsubmission.to_csv('resultados.csv', index=False)\n\nprint(\"Archivo de envío creado: resultados.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T02:24:33.854017Z","iopub.execute_input":"2025-01-05T02:24:33.854543Z","iopub.status.idle":"2025-01-05T02:28:19.886875Z","shell.execute_reply.started":"2025-01-05T02:24:33.854504Z","shell.execute_reply":"2025-01-05T02:28:19.885346Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **8. Visualización de Resultados**","metadata":{}},{"cell_type":"code","source":"# Matriz de Confusión\ny_true = y_resampled  # Reemplaza con los valores reales de tu conjunto de validación\ny_pred = ensemble_model.predict(X_resampled)  # Reemplaza con las predicciones de tu conjunto de validación\n\ncm = confusion_matrix(y_true, y_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\ndisp.plot(cmap='Blues')\nplt.title('Matriz de Confusión')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T02:40:24.641550Z","iopub.execute_input":"2025-01-05T02:40:24.642179Z","iopub.status.idle":"2025-01-05T02:40:56.678183Z","shell.execute_reply.started":"2025-01-05T02:40:24.642127Z","shell.execute_reply":"2025-01-05T02:40:56.676874Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Curva Precision-Recall\ny_scores = ensemble_model.predict_proba(X_resampled)[:, 1]  # Reemplaza con las probabilidades de tu conjunto de validación\n\nprecision, recall, thresholds = precision_recall_curve(y_true, y_scores)\nplt.plot(recall, precision, label='Precision-Recall Curve')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Curva Precision-Recall')\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T02:40:59.475341Z","iopub.execute_input":"2025-01-05T02:40:59.475965Z","iopub.status.idle":"2025-01-05T02:41:34.874009Z","shell.execute_reply.started":"2025-01-05T02:40:59.475923Z","shell.execute_reply":"2025-01-05T02:41:34.872926Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Curva ROC y AUC\nfpr, tpr, thresholds = roc_curve(y_true, y_scores)\nauc_score = roc_auc_score(y_true, y_scores)\nplt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc_score:.2f})')\nplt.plot([0, 1], [0, 1], linestyle='--', color='gray')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Curva ROC')\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T02:41:46.952139Z","iopub.execute_input":"2025-01-05T02:41:46.952542Z","iopub.status.idle":"2025-01-05T02:41:47.729641Z","shell.execute_reply.started":"2025-01-05T02:41:46.952510Z","shell.execute_reply":"2025-01-05T02:41:47.727955Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Heatmap del F1 Score por Clase\nreport = classification_report(y_true, y_pred, output_dict=True)\ndf_report = pd.DataFrame(report).transpose()\n\nsns.heatmap(df_report.loc[:, [\"precision\", \"recall\", \"f1-score\"]].iloc[:-1, :], annot=True, cmap=\"YlGnBu\")\nplt.title(\"Heatmap del F1 Score por Clase\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T02:41:53.969939Z","iopub.execute_input":"2025-01-05T02:41:53.970423Z","iopub.status.idle":"2025-01-05T02:41:55.969972Z","shell.execute_reply.started":"2025-01-05T02:41:53.970384Z","shell.execute_reply":"2025-01-05T02:41:55.968874Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n# Distribución de Probabilidades\nsns.histplot(y_scores, bins=50, kde=True)\nplt.title('Distribución de Probabilidades Predichas')\nplt.xlabel('Probabilidad')\nplt.ylabel('Frecuencia')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T02:41:58.451915Z","iopub.execute_input":"2025-01-05T02:41:58.452427Z","iopub.status.idle":"2025-01-05T02:42:01.142407Z","shell.execute_reply.started":"2025-01-05T02:41:58.452382Z","shell.execute_reply":"2025-01-05T02:42:01.141155Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Feature Importance\nfeature_importances = pd.DataFrame({\n    'Feature': numeric_features + list(preprocessor.transformers_[1][1]['onehot'].get_feature_names_out(categorical_features)),\n    'Importance': ensemble_model.named_estimators_['rf'].feature_importances_\n}).sort_values(by='Importance', ascending=False)\n\nplt.figure(figsize=(10, 20))  # Ajustar el tamaño de la figura\nplt.barh(feature_importances['Feature'], feature_importances['Importance'])\nplt.title('Importancia de las Características')\nplt.xlabel('Importancia')\nplt.ylabel('Características')\nplt.gca().invert_yaxis()  # Invertir el eje y para que las características más importantes estén en la parte superior\nplt.xticks(rotation=45, ha='right')  # Rotar las etiquetas del eje x\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T02:42:02.971254Z","iopub.execute_input":"2025-01-05T02:42:02.971750Z","iopub.status.idle":"2025-01-05T02:42:04.280178Z","shell.execute_reply.started":"2025-01-05T02:42:02.971713Z","shell.execute_reply":"2025-01-05T02:42:04.279172Z"}},"outputs":[],"execution_count":null}]}